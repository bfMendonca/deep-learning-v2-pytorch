{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Autoencoder\n",
    "\n",
    "Sticking with the MNIST dataset, let's add noise to our data and see if we can define and train an autoencoder to _de_-noise the images.\n",
    "\n",
    "<img src='notebook_ims/autoencoder_denoise.png' width=70%/>\n",
    "\n",
    "Let's get started by importing our libraries and getting the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MNIST datasets are hosted on yann.lecun.com that has moved under CloudFlare protection\n",
    "# Run this script to enable the datasets download\n",
    "# Reference: https://github.com/pytorch/vision/issues/1938\n",
    "from six.moves import urllib\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "urllib.request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# load the training and test datasets\n",
    "train_data = datasets.MNIST(root='~/.pytorch/MNIST_data/', train=True,\n",
    "                                   download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='~/.pytorch/MNIST_data/', train=False,\n",
    "                                  download=True, transform=transform)\n",
    "\n",
    "# Create training and test dataloaders\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdadd63eda0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "    \n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# get one image from the batch\n",
    "img = np.squeeze(images[0])\n",
    "\n",
    "fig = plt.figure(figsize = (5,5)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Denoising\n",
    "\n",
    "As I've mentioned before, autoencoders like the ones you've built so far aren't too useful in practice. However, they can be used to denoise images quite successfully just by training the network on noisy images. We can create the noisy images ourselves by adding Gaussian noise to the training images, then clipping the values to be between 0 and 1.\n",
    "\n",
    ">**We'll use noisy images as input and the original, clean images as targets.** \n",
    "\n",
    "Below is an example of some of the noisy images I generated and the associated, denoised images.\n",
    "\n",
    "<img src='notebook_ims/denoising.png' />\n",
    "\n",
    "\n",
    "Since this is a harder problem for the network, we'll want to use _deeper_ convolutional layers here; layers with more feature maps. You might also consider adding additional layers. I suggest starting with a depth of 32 for the convolutional layers in the encoder, and the same depths going backward through the decoder.\n",
    "\n",
    "#### TODO: Build the network for the denoising autoencoder. Add deeper and/or additional layers compared to the model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvDenoiser(\n",
      "  (cv11): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (cv21): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (cv31): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (t_conv30): ConvTranspose2d(8, 8, kernel_size=(3, 3), stride=(2, 2))\n",
      "  (t_conv31): ConvTranspose2d(8, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (t_conv32): ConvTranspose2d(16, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (cv41): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the NN architecture\n",
    "class ConvDenoiser(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvDenoiser, self).__init__()\n",
    "        ## encoder layers ##\n",
    "        \n",
    "        self.cv11 = nn.Conv2d( 1,  32,  3, 1, padding=1 )\n",
    "        self.cv21 = nn.Conv2d( 32, 64, 3, 1, padding=1 )\n",
    "        self.cv31 = nn.Conv2d( 64, 8, 3, 1, padding=1 )\n",
    "        \n",
    "        self.pool = nn.MaxPool2d( 2, 2, )\n",
    "        \n",
    "        #------------------------------------------------------------#\n",
    "\n",
    "        ## decoder layers ##\n",
    "        self.t_conv30 = nn.ConvTranspose2d(8, 8, 3, stride=2)\n",
    "        self.t_conv31 = nn.ConvTranspose2d(8, 16, 2, stride=2)\n",
    "        self.t_conv32 = nn.ConvTranspose2d(16, 32, 2, stride=2)\n",
    "        self.cv41 = nn.Conv2d( 32, 1, 3, 1, padding=1)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, x, show_shape=False):\n",
    "        ## encode ##\n",
    "        \n",
    "        x = F.relu( self.cv11( x ) )\n",
    "        x = self.pool( x ) # 28x28x1 - 14x14x32\n",
    "        x = F.relu( self.cv21( x ) )\n",
    "        x = self.pool( x ) # 14x14x32 - 7x7x64\n",
    "        x = F.relu( self.cv31( x ) ) #7x7x64 - 7x7x8\n",
    "        x = self.pool( ( x ) ) #7x7x8 - 3x3x8\n",
    "        \n",
    "        ## decode ##\n",
    "        x = F.relu( self.t_conv30( x ) ) #3x3x8 => 7x7x8\n",
    "        \n",
    "        x = F.relu( self.t_conv31( x ) ) #7x7x8 => 14x14x16\n",
    "        \n",
    "        x = F.relu( self.t_conv32( x ) ) #14x14x16 => 28x28x32\n",
    "        \n",
    "        x = F.sigmoid( self.cv41(x) )    #28x28x32 => 28x28x1\n",
    "        \n",
    "        \n",
    "#         x = F.upsample( x, scale_factor=2, mode='nearest')\n",
    "#         x = F.relu( self.cv31( x ) )\n",
    "#         x = F.relu( self.cv32( x ) )\n",
    "#         x = F.relu( self.cv33( x ) )\n",
    "        \n",
    "#         x = F.upsample( x, scale_factor=2, mode='nearest')\n",
    "#         x = F.relu( self.cv41( x ) )\n",
    "#         x = F.relu( self.cv42( x ) )\n",
    "#         x = F.sigmoid( self.cv43( x ) )\n",
    "                \n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model = ConvDenoiser()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training\n",
    "\n",
    "We are only concerned with the training images, which we can get from the `train_loader`.\n",
    "\n",
    ">In this case, we are actually **adding some noise** to these images and we'll feed these `noisy_imgs` to our model. The model will produce reconstructed images based on the noisy input. But, we want it to produce _normal_ un-noisy images, and so, when we calculate the loss, we will still compare the reconstructed outputs to the original images!\n",
    "\n",
    "Because we're comparing pixel values in input and output images, it will be best to use a loss that is meant for a regression task. Regression is all about comparing quantities rather than probabilistic values. So, in this case, I'll use `MSELoss`. And compare output images and input images as follows:\n",
    "```\n",
    "loss = criterion(outputs, images)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# specify loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bmendonca/dev_tools/miniconda3/envs/deep-learning/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.077296\n",
      "Epoch: 2 \tTraining Loss: 0.844591\n",
      "Epoch: 3 \tTraining Loss: 0.782341\n",
      "Epoch: 4 \tTraining Loss: 0.749972\n",
      "Epoch: 5 \tTraining Loss: 0.729109\n",
      "Epoch: 6 \tTraining Loss: 0.713842\n",
      "Epoch: 7 \tTraining Loss: 0.703833\n",
      "Epoch: 8 \tTraining Loss: 0.694913\n",
      "Epoch: 9 \tTraining Loss: 0.689178\n",
      "Epoch: 10 \tTraining Loss: 0.682289\n",
      "Epoch: 11 \tTraining Loss: 0.676108\n",
      "Epoch: 12 \tTraining Loss: 0.670816\n",
      "Epoch: 13 \tTraining Loss: 0.666279\n",
      "Epoch: 14 \tTraining Loss: 0.661564\n",
      "Epoch: 15 \tTraining Loss: 0.656914\n",
      "Epoch: 16 \tTraining Loss: 0.654272\n",
      "Epoch: 17 \tTraining Loss: 0.649555\n",
      "Epoch: 18 \tTraining Loss: 0.646813\n",
      "Epoch: 19 \tTraining Loss: 0.644164\n",
      "Epoch: 20 \tTraining Loss: 0.641724\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 20\n",
    "\n",
    "# for adding noise to images\n",
    "noise_factor=0.5\n",
    "\n",
    "\n",
    "with torch.cuda.device(0):\n",
    "    \n",
    "    model = model.cuda()\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # monitor training loss\n",
    "        train_loss = 0.0\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        for data in train_loader:\n",
    "            # _ stands in for labels, here\n",
    "            # no need to flatten images\n",
    "            images, _ = data\n",
    "\n",
    "            ## add random noise to the input images\n",
    "            noisy_imgs = images + noise_factor * torch.randn(*images.shape)\n",
    "            # Clip the images to be between 0 and 1\n",
    "            noisy_imgs = np.clip(noisy_imgs, 0., 1.)\n",
    "\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            noisy_imgs = noisy_imgs.cuda()\n",
    "            images = images.cuda()\n",
    "\n",
    "            ## forward pass: compute predicted outputs by passing *noisy* images to the model\n",
    "            outputs = model(noisy_imgs)\n",
    "\n",
    "            # calculate the loss\n",
    "            # the \"target\" is still the original, not-noisy images\n",
    "            loss = criterion(outputs, images)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update running training loss\n",
    "            train_loss += loss.item()*images.size(0)\n",
    "\n",
    "        # print avg training statistics \n",
    "        train_loss = train_loss/len(train_loader)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss\n",
    "            ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking out the results\n",
    "\n",
    "Here I'm adding noise to the test images and passing them through the autoencoder. It does a suprising great job of removing the noise, even though it's sometimes difficult to tell what the original number is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'noise_factor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-763346090c13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# add noise to the test images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnoisy_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnoise_factor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mnoisy_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoisy_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'noise_factor' is not defined"
     ]
    }
   ],
   "source": [
    "# obtain one batch of test images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# add noise to the test images\n",
    "noisy_imgs = images + noise_factor * torch.randn(*images.shape)\n",
    "noisy_imgs = np.clip(noisy_imgs, 0., 1.)\n",
    "\n",
    "# get sample outputs\n",
    "output = model(noisy_imgs.cuda())\n",
    "\n",
    "output = output.cpu()\n",
    "\n",
    "# prep images for display\n",
    "noisy_imgs = noisy_imgs.numpy()\n",
    "\n",
    "# output is resized into a batch of iages\n",
    "output = output.view(batch_size, 1, 28, 28)\n",
    "# use detach when it's an output that requires_grad\n",
    "output = output.detach().numpy()\n",
    "\n",
    "# plot the first ten input images and then reconstructed images\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))\n",
    "\n",
    "# input images on top row, reconstructions on bottom\n",
    "for noisy_imgs, row in zip([noisy_imgs, output], axes):\n",
    "    for img, ax in zip(noisy_imgs, row):\n",
    "        \n",
    "        \n",
    "        ax.imshow(np.squeeze(img), cmap='gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-udacity",
   "language": "python",
   "name": "deep-learning-udacity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
